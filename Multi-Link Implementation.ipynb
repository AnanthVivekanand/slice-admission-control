{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b23f4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "babfa59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RequestType:\n",
    "    def __init__(self, request_type, bandwidth, service_rate, arrival_rate, source, sink, distribution, switch_rate=None):\n",
    "        # distribution is 1x2 if elastic and 1x1 if static\n",
    "        \n",
    "        self.type = request_type\n",
    "        self.bw = bandwidth\n",
    "        self.service_rate = service_rate\n",
    "        self.arrival_rate = arrival_rate\n",
    "        self.source = source\n",
    "        self.sink = sink\n",
    "        self.distribution = distribution\n",
    "        self.switch_rate = switch_rate\n",
    "\n",
    "class Request:\n",
    "    def __init__(self, request_type, service_time, arrival_time, source, sink, transfer_rate, distribution=None, parent_elastic=None, bw_dist=None):\n",
    "        self.type = request_type\n",
    "        self.service_time = service_time\n",
    "        self.arrival_time = arrival_time\n",
    "        self.source = source\n",
    "        self.sink = sink\n",
    "        self.bw = transfer_rate\n",
    "        self.request_type = request_type\n",
    "        self.parent_elastic = parent_elastic\n",
    "        self.accepted = None\n",
    "        self.path = None\n",
    "        self.bw_dist = bw_dist\n",
    "        \n",
    "        if request_type == \"elastic\":\n",
    "            self.distribution = distribution\n",
    "            self.scale_requests = []\n",
    "            \n",
    "    def add_scale_request(self, req): \n",
    "        # we store related scale requests for elastic requests\n",
    "        # not used if static request\n",
    "        self.scale_requests.append(req)\n",
    "            \n",
    "    def get_encoding(self, nodes_in_environment):\n",
    "        # as per our notes, this SHOULD return 1x5 tensor,\n",
    "        # but we have one hot encodings INSIDE this tensor,\n",
    "        # so we will flatten this and return, so the size will be\n",
    "        # larger than 1x5\n",
    "        \n",
    "        # nodes_in_environment is a list of all the nodes in our graph\n",
    "        # eg [\"a\", \"b\", \"c\"]\n",
    "        \n",
    "        # request is [one hot source, one hot destination, bw, service time, one hot type]\n",
    "                \n",
    "        one_hot_source = nn.functional.one_hot(torch.tensor([nodes_in_environment.index(self.source)]), num_classes=len(nodes_in_environment)).flatten()\n",
    "        one_hot_dest   = nn.functional.one_hot(torch.tensor([nodes_in_environment.index(self.sink)]), num_classes=len(nodes_in_environment)).flatten()\n",
    "    \n",
    "        if self.request_type == \"static\" or self.request_type == \"scale\":\n",
    "            one_hot_type = torch.tensor([1, 0])\n",
    "        elif self.request_type == \"elastic\":\n",
    "            one_hot_type = torch.tensor([0, 1])\n",
    "            \n",
    "        encoding = torch.cat([one_hot_source, \n",
    "                             one_hot_dest,\n",
    "                             torch.tensor([self.bw]), \n",
    "                             torch.tensor([self.service_time]),\n",
    "                             one_hot_type])\n",
    "        \n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4c5db9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Link:\n",
    "    def __init__(self, node_1, node_2, bw_capacity):\n",
    "        self.serving_requests = []\n",
    "        self.nodes = [node_1, node_2]\n",
    "        self.total_bw = bw_capacity\n",
    "        \n",
    "    def reset(self):\n",
    "        self.serving_requests = []\n",
    "        \n",
    "    def add_request(self, request_obj):\n",
    "        self.serving_requests.append(request_obj)\n",
    "        \n",
    "    def remove_request(self, request_obj):\n",
    "        self.serving_requests.remove(request_obj)\n",
    "        \n",
    "    def remaining_bw(self): \n",
    "        # subtracting bw being used from total bw capacity\n",
    "        bw_being_used = 0\n",
    "        for req in self.serving_requests:\n",
    "            bw_being_used += req.bw\n",
    "            \n",
    "        return (self.total_bw - bw_being_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55c30a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    # requests_in_service_encoder = nn.RNN(????, 7)\n",
    "    \n",
    "    def __init__(self, nodes, links, request_blueprints):\n",
    "        \"\"\"\n",
    "        nodes: list of strings where each string is just a name or identifier of a node\n",
    "        links: list of tuples where in tuple t, t[0] is first node, t[1] is another node, and t[2] is bw capacity of the link\n",
    "        request_blueprints: list of DeploymentRequest objects\n",
    "        \"\"\"\n",
    "        self.nodes = nodes\n",
    "        self.links = {}\n",
    "        self.request_history = []\n",
    "        self.E_history = []\n",
    "        self.past_distributions = []\n",
    "        self.request_blueprints = request_blueprints\n",
    "        self.last_time = 0\n",
    "        self.episode_timesteps = 600\n",
    "        \n",
    "        for link in links:\n",
    "            if link[0] not in self.nodes or link[1] not in self.nodes:\n",
    "                raise Exception(\"Node in link \" + str(link) + \" doesn't exist\")\n",
    "            \n",
    "            link_obj = Link(*link)\n",
    "\n",
    "            self.links[link[0] + link[1]] = link_obj\n",
    "            self.links[link[1] + link[0]] = link_obj\n",
    "            \n",
    "        self.request_list = self.create_requests()\n",
    "        self.request_queue = iter(self.request_list)\n",
    "            \n",
    "    def add_request(self, request, path=None): # we want to add this request to a link or path\n",
    "        # path: a list of nodes that the request traverses including source and sink\n",
    "        # if no path is specified, path is assumed to be [req.source, req.sink]\n",
    "        \n",
    "        if path is not None: \n",
    "            nodes = [[path[i], path[i + 1]] for i in range(len(path) - 1)]\n",
    "            for node_pair in nodes:\n",
    "                env.links[node_pair[0] + node_pair[1]].add_request(request)\n",
    "        \n",
    "        else:\n",
    "            self.links[request.source + request.sink].add_request(request)\n",
    "        \n",
    "        request.accepted = True\n",
    "        self.request_history.append(request)\n",
    "        # print(self.links[request.source + request.sink])\n",
    "    \n",
    "    def reset(self):\n",
    "        for link in self.links.values():\n",
    "            link.reset()\n",
    "        self.request_history = []\n",
    "        self.E_history = []\n",
    "        self.past_distributions = []\n",
    "        self.last_time = 0\n",
    "        self.request_list = env.create_requests()\n",
    "        self.request_queue = iter(self.request_list)\n",
    "        \n",
    "        return env.get_encoding()\n",
    "        \n",
    "    def reward(self, request, decision):\n",
    "        base_rate = 1         # 1 when static\n",
    "        type_bonus = 0.9      # 0.9 when static\n",
    "        bw = request.bw\n",
    "        if request.type == \"elastic\":\n",
    "            base_rate = request.bw\n",
    "            type_bonus = 1.1                # 1.1 when elastic\n",
    "            bw = np.array(request.bw_dist).dot(request.distribution)\n",
    "            \n",
    "            \n",
    "        r = bw * base_rate * request.service_time * type_bonus\n",
    "        \n",
    "        # if remaining bandwidth on link(s) < 0, very \"bad\" reward\n",
    "        if request.path is not None:\n",
    "            path_length = len(request.path)\n",
    "            \n",
    "            r *= math.pow(0.9, path_length - 2)\n",
    "            \n",
    "            nodes = [[request.path[i], request.path[i + 1]] for i in range(len(request.path) - 1)]\n",
    "            for node_pair in nodes:\n",
    "                if self.links[node_pair[0] + node_pair[1]].remaining_bw() < 0:\n",
    "                    return (-r * 10)\n",
    "        else:\n",
    "            # path is direct, so no decrease of reward needed\n",
    "            remaining_bw = self.links[request.source + request.sink].remaining_bw()\n",
    "            if remaining_bw < 0:\n",
    "                return (-r * 10)\n",
    "        \n",
    "        if decision == \"accept\":\n",
    "            return r\n",
    "        \n",
    "        if decision == \"reject\":\n",
    "            if request.type == \"static\" or request.type == \"scale\":\n",
    "                return 0\n",
    "            elif request.type == \"elastic\":\n",
    "                current_sum = torch.from_numpy(np.sum(self.past_distributions, axis=0))\n",
    "\n",
    "                average_past_distribution = current_sum / len(self.past_distributions)\n",
    "                current_req_distribution = torch.tensor(request.distribution)\n",
    "                \n",
    "                return -1 * r * math.exp(-nn.functional.kl_div(average_past_distribution, current_req_distribution))\n",
    "                \n",
    "                \"\"\"\n",
    "                past_distributions = []\n",
    "                for req in self.request_history:\n",
    "                    if req.request_type == \"elastic\":\n",
    "                        past_distributions.append(req.distribution)\n",
    "                \n",
    "                average_past_distribution = torch.mean(past_distributions, dim=1)\n",
    "                current_req_distribution = torch.tensor(request.distribution)\n",
    "                \n",
    "                if bool(average_past_distribution[0] < current_req_distribution[0]):\n",
    "                    return -1 * r * math.exp(-nn.functional.kl_div(average_past_distribution, current_req_distribution))\n",
    "                else:\n",
    "                    return 0\n",
    "                \"\"\"\n",
    "                \n",
    "    def next_req(self):\n",
    "        return next(self.request_queue)\n",
    "                \n",
    "    def step(self, req, action):\n",
    "        # what happens if we have two requests that come in on the same timestep but there is only enough bandwidth for one?\n",
    "        # do we the decision on the second request with knowledge of the first request\n",
    "        # essentially, after we accept the first request, will we submit an updated encoding of the network to the policy network?\n",
    " \n",
    "        # actions is a Nx2 matrix where the first column in the request and second is the decision\n",
    "        # decision is either \"accept\" or \"reject\"\n",
    "        # this is given by our agent\n",
    "                \n",
    "        if action[0] > 0.5:\n",
    "            # accept request\n",
    "            paths = (env.search(req.source, req.sink, [], []))\n",
    "            paths.sort(key=lambda x: len(x)) # sort by shortest path\n",
    "            # select the path we are using\n",
    "            path = paths[action[1:4].argmax()]\n",
    "            \n",
    "            self.add_request(req, path)\n",
    "        \n",
    "            reward = env.reward(req, \"accept\")\n",
    "        elif action[0] < 0.5:\n",
    "            # reject\n",
    "            reward = env.reward(req, \"reject\")\n",
    "        \n",
    "        obs = env.get_encoding()\n",
    "        \n",
    "        done = req.arrival_time > 600\n",
    "        info = None\n",
    "        \n",
    "        return obs, reward, done, info\n",
    "        \n",
    "    def update_requests(self, current_time):\n",
    "        # here, we remove expired requests and update E_history based off of the request stats\n",
    "        \n",
    "        for link in self.links.values():\n",
    "            for request in link.serving_requests:\n",
    "                if (request.arrival_time + request.service_time) > self.last_time and (request.arrival_time + request.service_time) < current_time:\n",
    "                    # request has expired, let's remove it from the links\n",
    "                    for link in self.links.values():\n",
    "                        if request in link.serving_requests:\n",
    "                            link.remove_request(request)\n",
    "\n",
    "                    if request.type == \"elastic\":\n",
    "                        time_on_higher_bw = 0\n",
    "                        for scale_req in request.scale_requests:\n",
    "                            time_on_higher_bw += scale_req.service_time\n",
    "\n",
    "                        time_on_lower_bw = request.service_time - time_on_higher_bw\n",
    "\n",
    "                        # calculate E[history]\n",
    "                        request_time = np.array([time_on_lower_bw, time_on_higher_bw])\n",
    "                        request_bw = request.bw\n",
    "                        result = (request_time / request_time.sum()).dot(request_bw)\n",
    "                        self.past_distributions.append(request_time / request_time.sum())\n",
    "                        self.E_history.append(result)\n",
    "\n",
    "    def get_encoding(self):\n",
    "        links_processed = [] \n",
    "        # these will store links that we have already encoded so we don't encode them again\n",
    "        \n",
    "        current_encoding = []\n",
    "        \n",
    "        # h = torch.zeros(7) # assuming 7 for h0 size\n",
    "        # last_out = None\n",
    "        \n",
    "        env_encoding = []\n",
    "        \n",
    "        next_req = self.next_req()\n",
    "        \n",
    "        \"\"\"\n",
    "        while next_req.type == \"scale\":\n",
    "            if next_req.parent_elastic.accepted:\n",
    "                next_req.accepted = True # we must accept since we accepted elastic req\n",
    "                self.add_request(next_req, next_req.parent_elastic.path)\n",
    "            next_req = self.next_req()\n",
    "        \"\"\"\n",
    "            \n",
    "        self.update_requests(next_req.arrival_time)\n",
    "        \n",
    "        for link in self.links.values():\n",
    "            if link in links_processed:\n",
    "                continue\n",
    "\n",
    "                        \n",
    "            # Commented because we don't want to encode any queue for phase 1\n",
    "            \n",
    "            # for req in link.serving_requests\n",
    "                # request is [one hot source, one hot destination, bw, service time, one hot type]\n",
    "                \n",
    "                # one_hot_source = nn.functional.one_hot(torch.tensor([self.nodes.index(req.source)]), num_classes=len(self.nodes))\n",
    "                # one_hot_dest   = nn.functional.one_hot(torch.tensor([self.nodes.index(req.sink)]), num_classes=len(self.nodes))\n",
    "\n",
    "                # req_tensor = torch.Tensor([]) # mismatched dimensions??!\n",
    "                # last_out, h = self.requests_in_service_encoder(req_tensor, h)\n",
    "\n",
    "            # current_encoding.append(torch.cat(torch.Tensor([link.remaining_bw]), last_out))\n",
    "            # torch.stack(current_encoding)\n",
    "            \n",
    "            # check implementation later\n",
    "            \n",
    "            env_encoding.append(link.remaining_bw())\n",
    "            \n",
    "            links_processed.append(link)\n",
    "            \n",
    "        return torch.tensor(env_encoding), torch.tensor(next_req.get_encoding(env.nodes)), next_req\n",
    "    \n",
    "    def create_requests(self):\n",
    "        requests = []\n",
    "        \n",
    "        for request_type in self.request_blueprints:\n",
    "            arrival_times = []\n",
    "            service_times = []\n",
    "            last_arrival = 0\n",
    "        \n",
    "            while last_arrival < self.episode_timesteps: # we want to generate requests till we reach episode end\n",
    "                last_arrival += np.random.exponential(request_type.arrival_rate)\n",
    "                arrival_times.append(last_arrival)\n",
    "                                \n",
    "            for _ in arrival_times:\n",
    "                service_times.append(np.random.exponential(request_type.service_rate))\n",
    "                \n",
    "            for arrival_time, service_time in zip(arrival_times, service_times):\n",
    "                # start creating requests\n",
    "                \n",
    "                new_request = Request(request_type.type, service_time, arrival_time, request_type.source, request_type.sink, request_type.bw[0], request_type.distribution, bw_dist=request_type.bw)\n",
    "                requests.append(new_request)\n",
    "                \n",
    "                if request_type.type == \"elastic\": \n",
    "                    # we will start with the first bandwidth element as starting bw\n",
    "                    # WE ASSUME that bw[0] < bw[1]\n",
    "                    timesteps_from_deployment = 0\n",
    "                    current_bw = request_type.bw[0]\n",
    "                    while timesteps_from_deployment < service_time:\n",
    "                        if current_bw == request_type.bw[0]:\n",
    "                            # we want to generate a scale request to increase bw\n",
    "                            scale_bw = request_type.bw[1] - current_bw\n",
    "                            scale_service_time = np.random.exponential(request_type.switch_rate[1])\n",
    "                            scale_request = Request(\"scale\", scale_service_time, \\\n",
    "                                                    arrival_time + timesteps_from_deployment, request_type.source, \\\n",
    "                                                   request_type.sink, scale_bw, parent_elastic=new_request)\n",
    "                            requests.append(scale_request)\n",
    "                            new_request.add_scale_request(scale_request)\n",
    "                            \n",
    "                            timesteps_from_deployment += scale_service_time\n",
    "                            current_bw = request_type.bw[1] # request_type.bw[0] + scale_bw\n",
    "                        elif current_bw == request_type.bw[1]:\n",
    "                            # we want to go to lower bw and spend some time there\n",
    "                            time_spent_on_lower_bw = np.random.exponential(request_type.switch_rate[0])\n",
    "                            timesteps_from_deployment += time_spent_on_lower_bw\n",
    "                            current_bw = request_type.bw[0]\n",
    "                            \n",
    "        # sort requests by arrival time\n",
    "        requests.sort(key=lambda x: x.arrival_time)\n",
    "        return requests\n",
    "    \n",
    "    def search(self, source, dest, visited_a, paths):\n",
    "        visited_a.append(source)\n",
    "        # print(visited_a)\n",
    "\n",
    "        for link in set(env.links.values()):\n",
    "            visited = visited_a.copy()\n",
    "            if source in link.nodes:\n",
    "                if dest in link.nodes:\n",
    "                    visited.append(dest)\n",
    "                    paths.append(visited)\n",
    "\n",
    "                x = link.nodes.copy()\n",
    "                x.remove(source)\n",
    "                if x[0] not in visited:\n",
    "                    self.search(x[0], dest, visited.copy(), paths)\n",
    "        return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ba89a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment([\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"], [[\"a\", \"b\", 10], [\"a\", \"c\", 10], [\"b\", \"d\", 10], \\\n",
    "                                                   [\"c\", \"d\", 20], [\"c\", \"e\", 10], [\"d\", \"f\", 10], \\\n",
    "                                                   [\"e\", \"f\", 10]], \\\n",
    "                  [RequestType(\"static\", [2], 0.5, 0.75, \"a\", \"b\", [1]), \\\n",
    "                  RequestType(\"static\", [8], 1, 1.5, \"a\", \"b\", [1]), \\\n",
    "                  RequestType(\"elastic\", [4, 9], 1, 1.5, \"a\", \"b\", [0.8, 0.2], switch_rate=[0.08, 0.02]), \\\n",
    "                  RequestType(\"static\", [1], 1, 1.5, \"c\", \"d\", [1]), \\\n",
    "                  RequestType(\"static\", [7], 0.5, 0.75, \"c\", \"d\", [1]), \\\n",
    "                  RequestType(\"elastic\", [3, 13], 2, 3, \"c\", \"d\", [0.9, 0.1], switch_rate=[0.09, 0.01]), \\\n",
    "                  RequestType(\"static\", [3], 0.5, 0.75, \"e\", \"f\", [1]), \\\n",
    "                   RequestType(\"static\", [6], 1, 1.5, \"e\", \"f\", [1]), \\\n",
    "                    RequestType(\"elastic\", [5, 8], 2, 3, \"e\", \"f\", [0.7, 0.3], switch_rate=[0.07, 0.03])])\n",
    "\n",
    "\n",
    "                # self, request_type, bandwidth, service_rate, arrival_rate, source, sink, distribution, switch_rate=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c14ece7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(env_encoding, next_req_encoding, next_req_obj):    \n",
    "    # find all paths between source and sink\n",
    "    paths = (env.search(next_req_obj.source, next_req_obj.sink, [], []))\n",
    "    paths.sort(key=lambda x: len(x)) # sort by shortest path\n",
    "    selection = 0\n",
    "    for path in paths:\n",
    "        # check if this path works\n",
    "        works = True\n",
    "        nodes = [[path[i], path[i + 1]] for i in range(len(path) - 1)]\n",
    "        for node_pair in nodes:\n",
    "            if env.links[node_pair[0] + node_pair[1]].remaining_bw() < next_req_obj.bw:\n",
    "                works = False\n",
    "                \n",
    "        if works:\n",
    "            selection = paths.index(path)\n",
    "            selection_one_hot = nn.functional.one_hot(torch.tensor([selection]), num_classes=3).flatten()\n",
    "            next_req_obj.path = path\n",
    "            return torch.cat([torch.tensor([1]), selection_one_hot])\n",
    "        \n",
    "    return torch.cat([torch.tensor([0]), torch.tensor([0,0,0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bebc2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-80c915bbf2d2>:222: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(env_encoding), torch.tensor(next_req.get_encoding(env.nodes)), next_req\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18027.526940416392\n"
     ]
    }
   ],
   "source": [
    "total_reward = 0\n",
    "env_encoding, next_req_encoding, next_req_obj = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    decision = policy(env_encoding, next_req_encoding, next_req_obj)\n",
    "    \n",
    "    obs, reward, done, info = env.step(next_req_obj, decision)\n",
    "    env_encoding, next_req_encoding, next_req_obj = obs\n",
    "    \n",
    "    total_reward += reward\n",
    "\n",
    "print(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e50143",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009d736a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b16b86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f11071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c831713b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365fa714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992a452f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4cb718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86090cd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3b9047",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f37fee5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
