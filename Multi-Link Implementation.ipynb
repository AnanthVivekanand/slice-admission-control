{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b23f4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "babfa59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RequestType:\n",
    "    def __init__(self, request_type, service_rate, arrival_rate, source, sink, distribution, switch_rate=None):\n",
    "        # distribution is 1x2 if elastic and 1x1 if static\n",
    "        \n",
    "        self.type = request_type\n",
    "        self.service_rate = service_rate\n",
    "        self.arrival_rate = arrival_rate\n",
    "        self.source = source\n",
    "        self.sink = sink\n",
    "        self.distribution = distribution\n",
    "        self.switch_rate = switch_rate\n",
    "\n",
    "class Request:\n",
    "    def __init__(self, request_type, service_time, arrival_time, source, sink, transfer_rate, distribution=None):\n",
    "        self.type = request_type\n",
    "        self.service_time = service_time\n",
    "        self.arrival_time = arrival_time\n",
    "        self.source = source\n",
    "        self.sink = sink\n",
    "        self.bw = transfer_rate\n",
    "        self.request_type = request_type\n",
    "        \n",
    "        if request_type == \"elastic\":\n",
    "            self.distribution = distribution\n",
    "            \n",
    "    def get_encoding(self, nodes_in_environment):\n",
    "        # as per our notes, this SHOULD return 1x5 tensor,\n",
    "        # but we have one hot encodings INSIDE this tensor,\n",
    "        # so we will flatten this and return, so the size will be\n",
    "        # larger than 1x5\n",
    "        \n",
    "        # nodes_in_environment is a list of all the nodes in our graph\n",
    "        # eg [\"a\", \"b\", \"c\"]\n",
    "        \n",
    "        # request is [one hot source, one hot destination, bw, service time, one hot type]\n",
    "                \n",
    "        one_hot_source = nn.functional.one_hot(torch.tensor([nodes_in_environment.index(self.source)]), num_classes=len(nodes_in_environment))\n",
    "        one_hot_dest   = nn.functional.one_hot(torch.tensor([nodes_in_environment.index(self.sink)]), num_classes=len(nodes_in_environment))\n",
    "\n",
    "        if self.request_type == \"static\":\n",
    "            one_hot_type = torch.tensor([[1, 0]])\n",
    "        elif self.request_type == \"elastic\":\n",
    "            one_hot_type = torch.tensor([[0, 1]])\n",
    "            \n",
    "        encoding = torch.cat((one_hot_source, \n",
    "                             one_hot_dest,\n",
    "                             torch.tensor([[self.bw]]), # develop fix for when bw is already a tensor (when we have elastic request)\n",
    "                             torch.tensor([[self.service_time]]),\n",
    "                             one_hot_type), axis=1)\n",
    "        \n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4c5db9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Link:\n",
    "    def __init__(self, node_1, node_2, bw_capacity):\n",
    "        self.serving_requests = []\n",
    "        self.nodes = [node_1, node_2]\n",
    "        self.total_bw = bw_capacity\n",
    "        \n",
    "    def reset(self):\n",
    "        self.serving_requests = []\n",
    "        \n",
    "    def add_request(self, request_obj):\n",
    "        self.serving_requests.append(request_obj)\n",
    "        \n",
    "    def remaining_bw(self): \n",
    "        # subtracting bw being used from total bw capacity\n",
    "        bw_being_used = 0\n",
    "        for req in self.serving_requests:\n",
    "            bw_being_used += req.bw\n",
    "            \n",
    "        return (self.total_bw - bw_being_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55c30a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    # requests_in_service_encoder = nn.RNN(????, 7)\n",
    "    \n",
    "    def __init__(self, nodes, links, request_blueprints):\n",
    "        \"\"\"\n",
    "        nodes: list of strings where each string is just a name or identifier of a node\n",
    "        links: list of tuples where in tuple t, t[0] is first node, t[1] is another node, and t[2] is bw capacity of the link\n",
    "        request_blueprints: list of DeploymentRequest objects\n",
    "        \"\"\"\n",
    "        self.nodes = nodes\n",
    "        self.links = {}\n",
    "        self.request_history = []\n",
    "        self.request_blueprints = request_blueprints\n",
    "        \n",
    "        for link in links:\n",
    "            if link[0] not in self.nodes or link[1] not in self.nodes:\n",
    "                raise Exception(\"Node in link \" + str(link) + \" doesn't exist\")\n",
    "            \n",
    "            link_obj = Link(*link)\n",
    "\n",
    "            self.links[link[0] + link[1]] = link_obj\n",
    "            self.links[link[1] + link[0]] = link_obj\n",
    "            \n",
    "            \n",
    "    def add_request(self, request): # we want to add this request to a link\n",
    "        self.links[request.source + request.sink].add_request(request)\n",
    "        self.request_history.append(request)\n",
    "        print(self.links[request.source + request.sink])\n",
    "    \n",
    "    def reset(self):\n",
    "        self.nodes = nodes\n",
    "        for link in self.links.values():\n",
    "            link.reset()\n",
    "        \n",
    "        # also return initial observation, implement this later once we have function\n",
    "        # for encoding network\n",
    "        \n",
    "    def reward(self, request, decision):\n",
    "        base_rate = 1         # 1 when static\n",
    "        type_bonus = 0.9      # 0.9 when static\n",
    "        if request.type == \"elastic\":\n",
    "            base_rate = request.base_rate  \n",
    "            type_bonus = 1.1                # 1.1 when elastic\n",
    "            \n",
    "        r = request.bandwidth * base_rate * request.service_time * type_bonus\n",
    "        \n",
    "        # if remaining bandwidth on link < 0, very \"bad\" reward\n",
    "        remaining_bw = self.links[request.nodes[0] + request.nodes[1]].remaining_bw()\n",
    "        if remaining_bw < 0:\n",
    "            return (-r * 10)\n",
    "        \n",
    "        if decision == \"accept\":\n",
    "            return r\n",
    "        \n",
    "        if decision == \"reject\":\n",
    "            if request.type == \"static\":\n",
    "                return 0\n",
    "            elif request.type == \"elastic\":\n",
    "                past_distributions = []\n",
    "                for req in self.request_history:\n",
    "                    if req.request_type == \"elastic\":\n",
    "                        past_distributions.append(req.distribution)\n",
    "                \n",
    "                average_past_distribution = torch.mean(past_distributions, dim=1)\n",
    "                current_req_distribution = torch.tensor(request.distribution)\n",
    "                \n",
    "                if bool(average_past_distribution[0] < current_req_distribution[0]):\n",
    "                    return -1 * r * math.exp(-nn.functional.kl_div(average_past_distribution, current_req_distribution))\n",
    "                else:\n",
    "                    return 0\n",
    "                \n",
    "    #def step(self, action):\n",
    "        # what happens if we have two requests that come in on the same timestep but there is only enough bandwidth for one?\n",
    "        # do we the decision on the second request with knowledge of the first request\n",
    "        # essentially, after we accept the first request, will we submit an updated encoding of the network to the policy network?\n",
    " \n",
    "        # actions is a Nx2 matrix where the first column in the request and second is the decision\n",
    "        # decision is either \"accept\" or \"reject\"\n",
    "        \n",
    "        \n",
    "\n",
    "    def get_encoding(self):\n",
    "        links_processed = [] \n",
    "        # these will store links that we have already encoded so we don't encode them again\n",
    "        \n",
    "        current_encoding = []\n",
    "        \n",
    "        # h = torch.zeros(7) # assuming 7 for h0 size\n",
    "        # last_out = None\n",
    "        \n",
    "        env_encoding = []\n",
    "        \n",
    "        for link in self.links.values():\n",
    "            if link in links_processed:\n",
    "                continue\n",
    "\n",
    "                        \n",
    "            # Commented because we don't want to encode any queue for phase 1\n",
    "            \n",
    "            # for req in link.serving_requests\n",
    "                # request is [one hot source, one hot destination, bw, service time, one hot type]\n",
    "                \n",
    "                # one_hot_source = nn.functional.one_hot(torch.tensor([self.nodes.index(req.source)]), num_classes=len(self.nodes))\n",
    "                # one_hot_dest   = nn.functional.one_hot(torch.tensor([self.nodes.index(req.sink)]), num_classes=len(self.nodes))\n",
    "\n",
    "                # req_tensor = torch.Tensor([]) # mismatched dimensions??!\n",
    "                # last_out, h = self.requests_in_service_encoder(req_tensor, h)\n",
    "\n",
    "            # current_encoding.append(torch.cat(torch.Tensor([link.remaining_bw]), last_out))\n",
    "            # torch.stack(current_encoding)\n",
    "            \n",
    "            # check implementation later\n",
    "            \n",
    "            env_encoding.append(link.remaining_bw())\n",
    "            \n",
    "            links_processed.append(link)\n",
    "            \n",
    "        return torch.tensor(env_encoding)\n",
    "    \n",
    "    def create_requests():\n",
    "        requests = []\n",
    "        \n",
    "        for request_type in self.request_blueprints:\n",
    "            arrival_times = []\n",
    "            service_times = []\n",
    "            last_arrival = 0\n",
    "            episode_timesteps = 600\n",
    "        \n",
    "            while last_arrival < episode_timesteps: # we want to generate requests till we reach episode end\n",
    "                last_arrival += np.random.exponential(request_type.arrival_rate)\n",
    "                arrival_times.append(last_arrival)\n",
    "                \n",
    "            for _ in arrival_times:\n",
    "                service_times.append(np.random.exponential(request_type.service_rate))\n",
    "                \n",
    "            for arrival_time, service_time in zip(arrival_times, service_times):\n",
    "                # start creating requests\n",
    "                \n",
    "                new_request = Request(request_type.type, service_time, arrival_time, request_type.source, request_type.sink, request_type.distribution[0])\n",
    "                \n",
    "                if request_type.type == \"elastic\": \n",
    "                    # we will start with the first distribution element as starting bw\n",
    "                    # WE ASSUME that distribution[0] < distribution[1]\n",
    "                    timesteps_from_deployment = 0\n",
    "                    current_bw = request_type.distribution[0]\n",
    "                    while timesteps_from_deployment < service_time:\n",
    "                        if current_bw == request_type.distribution[0]:    \n",
    "                            # we want to generate a scale request to increase bw\n",
    "                            scale_bw = request_type.distribution[1] - current_bw\n",
    "                            scale_service_time = np.random.exponential(request_type.switch_rate[0])\n",
    "                            scale_request = Request(request_type.type, scale_service_time, \\\n",
    "                                                    last_arrival + timesteps_from_deployment, request_type.source, \\\n",
    "                                                   request_type.sink, scale_bw)\n",
    "                            requests.append(scale_request)\n",
    "                            \n",
    "                            timesteps_from_deployment += scale_service_time\n",
    "                            current_bw = request_type.distribution[0] + scale_bw # also equal to request_type.distribution[1]\n",
    "                        elif current_bw == request_type.distribution[1]:\n",
    "                            # we want to go to lower bw and spend some time there\n",
    "                            time_spent_on_lower_bw = np.random.exponential(request_type.switch_rate[1])\n",
    "                            timesteps_from_deployment += time_spent_on_lower_bw\n",
    "                            current_bw = request_type.distribution[0]\n",
    "                            \n",
    "        # sort requests by arrival time\n",
    "        requests.sort(key=lambda x: x.arrival_time)\n",
    "        return requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ba89a96",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'request_blueprints'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-674721aaa87d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"b\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"static\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"static\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'request_blueprints'"
     ]
    }
   ],
   "source": [
    "env = Environment([\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"], [[\"a\", \"b\", 10], [\"a\", \"c\", 10], [\"b\", \"d\", 10], \\\n",
    "                                                   [\"c\", \"d\", 0]\n",
    "                                                   \n",
    "                                                   [RequestType(\"static\", 5, 5, \"a\", \"b\", [], [])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "009d736a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ab': <__main__.Link at 0x1123602e0>,\n",
       " 'ba': <__main__.Link at 0x1123602e0>,\n",
       " 'ac': <__main__.Link at 0x112360df0>,\n",
       " 'ca': <__main__.Link at 0x112360df0>,\n",
       " 'bc': <__main__.Link at 0x112360370>,\n",
       " 'cb': <__main__.Link at 0x112360370>}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.links # notice that there are pairs that point to the same link obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b9b16b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([17, 20, 15])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.get_encoding() # this should return the remaining bandwidth on all links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "06f11071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3])\n",
      "torch.Size([1, 1])\n",
      "tensor([[1, 0, 0]])\n",
      "tensor([5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 1, 0, 5, 5, 1, 0]])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.links['ab'].serving_requests[0].get_encoding(env.nodes) # sample encoding of a request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c831713b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = torch.tensor([[-1, 1], [1, 1]]).float()\n",
    "bool(torch.mean(d, dim=1)[0] < torch.tensor([0.01, 1])[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea58afd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992a452f",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f4cb718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3463404498842957\n",
      "15.050607154587606\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "print(np.random.exponential(5))\n",
    "print(np.random.exponential(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de249f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
